{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 - Building a Random Forest\n",
    "\n",
    "## Student: Asahi Cantu 253964"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statistics import median, mode, mean\n",
    "from collections import Counter\n",
    "from enum import Enum\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some simple type definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttrType(Enum):\n",
    "    cat = 0  # categorical (qualitative) attribute\n",
    "    num = 1  # numerical (quantitative) attribute\n",
    "\n",
    "\n",
    "class NodeType(Enum):\n",
    "    internal = 1\n",
    "    root = 0\n",
    "    leaf = 2\n",
    "\n",
    "\n",
    "class SplitType(Enum):\n",
    "    bin = 0  # binary split\n",
    "    multi = 1  # multi-way split\n",
    "\n",
    "\n",
    "class OperatorType(Enum):\n",
    "    leq = 0  # Less than or equal to.. <=\n",
    "    gt = 1  # Greater Than... >\n",
    "    eq = 2 # Equal to =\n",
    " \n",
    "\n",
    "class Attribute(object):\n",
    "    def __init__(self, label, type,is_target,stat):\n",
    "        self.label = label\n",
    "        self.stat = stat  # holds mean for numerical and mode for categorical attributes\n",
    "        self.is_target = is_target\n",
    "        self.type = type\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.label}:{self.type}\"\n",
    "    \n",
    "    def __unicode__(self):\n",
    "        return f\"{self.label}:{self.type}\"\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"{self.label}:{self.type}\"\n",
    "\n",
    "\n",
    "class Splitting(object):\n",
    "    def __init__(self, attr, infogain, split_type, cond, splits):\n",
    "        self.attr = attr  # attribute ID (index in ATTR)\n",
    "        self.infogain = infogain  # information gain if splitting is done on this attribute\n",
    "        self.split_type = split_type  # one of SplitType\n",
    "        self.cond = cond  # splitting condition, i.e., values on outgoing edges\n",
    "        # list of training records (IDs) for each slitting condition\n",
    "        self.splits = splits\n",
    "\n",
    "\n",
    "class Node(object):\n",
    "    def __init__(self, id, type,attr, parent_id, children=None, edge_value=None, val=None, split_type=None,operator_type = None, split_cond=None,\n",
    "                 infogain=None):\n",
    "        self.id = id  # ID (same as the index in DT.model list)\n",
    "        self.type = type  # one of NodeType\n",
    "        self.parent_id = parent_id  # ID of parent node (None if root)\n",
    "        self.children = children  # list of IDs of child nodes\n",
    "        self.attr = attr\n",
    "        self.operator_type = operator_type\n",
    "        # the value of the incoming edge (only if not root node)\n",
    "        self.edge_value = edge_value\n",
    "        self.val = val  # if root or internal node: the attribute that is compared at that node; if leaf node: the target value\n",
    "        self.split_type = split_type  # one of SplitType\n",
    "        # splitting condition (median value for binary splits on numerical values; otherwise a list of categorical values (corresponding to child nodes))\n",
    "        self.split_cond = split_cond\n",
    "        self.infogain = infogain\n",
    "        self.str = f\"id: {self.id}type:{self.type},Attr:{self.attr} EdgeValue:{self.edge_value}, Value={self.val} Type:{self.split_type} Conditions={self.split_cond} Gain:{self.infogain}\" \n",
    "\n",
    "    def operator_type_str(self):\n",
    "        if self.operator_type == OperatorType.leq:\n",
    "            return \"<=\"\n",
    "        if self.operator_type == OperatorType.gt:\n",
    "            return \">\"\n",
    "        if self.operator_type == OperatorType.eq:\n",
    "            return \"=\"\n",
    "    \n",
    "\n",
    "    def __str__(self):\n",
    "        return self.str\n",
    "    \n",
    "    def __unicode__(self):\n",
    "        return self.str\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.str\n",
    "\n",
    "    def append_child(self, node_id):\n",
    "        self.children.append(node_id)\n",
    "\n",
    "\n",
    "class DT(object):\n",
    "    def __init__(self, data,attributes,sampling_attr_size):\n",
    "        self.data = data  # training data set (loaded into memory)\n",
    "        self.model = None  # decision tree model\n",
    "        self.default_target = 0 # default target class\n",
    "        self.attributes = attributes\n",
    "        self.target_attribute = [x for x in self.attributes if  x.is_target ][0]\n",
    "        self.default_target = self.attributes.index(self.target_attribute)\n",
    "        self.sampling_attr_size = sampling_attr_size \n",
    "\n",
    "    def __subsampling_attributes(self,attrs):\n",
    "        attribute_length = len(attrs)\n",
    "        attr_indexes = np.random.choice(attribute_length,self.sampling_attr_size,replace=False).tolist()\n",
    "        #attr_indexes.append(attribute_length -1) #append target Node\n",
    "        return [attrs[i] for i in attr_indexes if i != attrs.index(self.target_attribute)]\n",
    "\n",
    "    def calculate_entropy(self,subset):\n",
    "        entropy = 0.0\n",
    "        sample_size = len(subset)\n",
    "        target_col = subset[self.target_attribute.label]\n",
    "        unique_vals = target_col.unique()\n",
    "        for unique_val in unique_vals:\n",
    "            unique_val_count = target_col[target_col == unique_val].count()\n",
    "            p_unique_val = unique_val_count / sample_size\n",
    "            entropy += -p_unique_val * math.log(p_unique_val,2)\n",
    "        return entropy\n",
    "\n",
    "    def calculate_gain(self,subset,attr):\n",
    "        entropy = self.calculate_entropy(subset)\n",
    "        sample_size = len(subset)\n",
    "        attr_col = subset[attr.label]\n",
    "        unique_vals = attr_col.unique()\n",
    "        for unique_val in unique_vals:\n",
    "            subset_data = subset[subset[attr.label] == unique_val]\n",
    "            entropy_val = self.calculate_entropy(subset_data)\n",
    "            unique_val_count = attr_col[attr_col == unique_val].count()\n",
    "            p_unique_val = unique_val_count / sample_size\n",
    "            entropy += -p_unique_val * entropy_val\n",
    "        return entropy\n",
    "\n",
    "\n",
    "    def __mean_squared_error(self, records):\n",
    "        \"\"\"\n",
    "        Calculates mean squared error for a selection of records.\n",
    "\n",
    "        :param records: Data records (given by indices)\n",
    "        \"\"\"\n",
    "        result = 0.0\n",
    "        if records.empty:\n",
    "            return result\n",
    "        mean = records.mean()\n",
    "        for record in records:\n",
    "            result += (record - mean)**2\n",
    "        return result/len(records)\n",
    "\n",
    "    def __find_best_attr(self,attrs, subset):\n",
    "        \"\"\"\n",
    "        Finds the attribute with the largest gain.\n",
    "        :param attrs: Set of attributes\n",
    "        :param subset: Training set (Pandas dataFrame with corresponding subset)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        #mse_p = self.__mean_squared_error(subset)  # parent's MSE\n",
    "        splittings = []  # holds the splitting information for each attribute\n",
    "        split_mode = None\n",
    "        for attr in attrs:\n",
    "            #attr_idx = attrs.index(attr)\n",
    "            splits = {}  # record IDs corresponding to each split\n",
    "            # splitting condition depends on the attribute type\n",
    "            if attr.is_target :# skip target attribute\n",
    "                continue\n",
    "            elif attr.type == AttrType.cat:  # categorical attribute, multi-way split on each possible value\n",
    "                split_mode = SplitType.multi\n",
    "                split_cond = subset[attr.label].unique()\n",
    "            elif attr.type == AttrType.num:  # numerical attribute => binary split on median value\n",
    "                split_mode = SplitType.bin\n",
    "                split_cond = subset[attr.label].mean()\n",
    "                # unique_vals = self.data[attr.label].unique()\n",
    "                # unique_vals.sort()\n",
    "                # med_vals = {}\n",
    "                # for i in range(0,unique_vals.size,2):\n",
    "                #     med_val = (unique_vals[i] + unique_vals[i]) /2\n",
    "                #     ss = subset[ subset[attr.label] <= med_val]\n",
    "                #     gain = self.calculate_gain(ss, attr)\n",
    "                #     med_vals.update({med_val:gain})\n",
    "                \n",
    "                #split_cond = sorted(med_vals.items(), key=lambda x: x[1], reverse=True)[0][0]\n",
    "                #print(f\"\\t best split cond is {split_cond}\")\n",
    "                #print(f\"\\t Calculating gain... {split_cond}\")\n",
    "                #split_cond = self.data.iloc[:,attr_idx].median()\n",
    "            infogain = self.calculate_gain(subset, attr)\n",
    "            splitting = Splitting(attr, infogain, split_mode, split_cond, splits)\n",
    "            splittings.append(splitting)\n",
    "\n",
    "        # find best splitting\n",
    "        best_splitting = sorted(splittings, key=lambda x: x.infogain, reverse=True)\n",
    "        return best_splitting[0]\n",
    "\n",
    "    def __add_node(self, parent_id,attr, node_type=NodeType.internal, edge_value=None, val=None, split_type=None,\n",
    "                   operator_type=None,split_cond=None):\n",
    "        \"\"\"\n",
    "        Adds a node to the decision tree.\n",
    "\n",
    "        :param parent_id:\n",
    "        :param node_type:opera\n",
    "        :param edge_value:\n",
    "        :param val:\n",
    "        :param split_type:\n",
    "        :param split_cond:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        node_id = len(self.model)  # id of the newly assigned node\n",
    "        if not self.model:  # the tree is empty\n",
    "            node_type = NodeType.root\n",
    "\n",
    "        node = Node(\n",
    "            node_id,\n",
    "            node_type,\n",
    "            attr, \n",
    "            parent_id, \n",
    "            children=[], \n",
    "            edge_value=edge_value, \n",
    "            val=val,\n",
    "            split_type=split_type,\n",
    "            operator_type=operator_type,\n",
    "            split_cond=split_cond)\n",
    "        self.model.append(node)\n",
    "\n",
    "        # also add it as a child of the parent node\n",
    "        if parent_id is not None:\n",
    "            self.model[parent_id].append_child(node_id)\n",
    "\n",
    "        return node_id\n",
    "    \n",
    "    def __id3(self, attrs,subset, parent_id=None,edge_value=None, value=None,operator_type = None,subsample_trees = False):\n",
    "        \"\"\"\n",
    "        Function ID3 that returns a decision tree.\n",
    "\n",
    "        :param attrs: Set of attributes\n",
    "        :param records: Training set (list of record ids)\n",
    "        :param parent_id: ID of parent node\n",
    "        :param value: Value corresponding to the parent attribute, i.e., label of the edge on which we arrived to this node\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        sample_attrs = attrs\n",
    "        if subsample_trees:\n",
    "            sample_attrs = self.__subsampling_attributes(attrs)\n",
    "\n",
    "        #print(f\"{subset.columns}\")\n",
    "\n",
    "        # empty training set or empty set of attributes => create leaf node with default class\n",
    "        if subset.empty or not sample_attrs or len(sample_attrs) == 0:\n",
    "            self.__add_node(parent_id,None, node_type=NodeType.leaf, edge_value=edge_value, val=value,operator_type=operator_type)\n",
    "            return\n",
    "        # if all records have the same target value => create leaf node with that target value\n",
    "        if len(attrs)  == 1:\n",
    "            node_value = subset[self.target_attribute.label].mean()\n",
    "            self.__add_node(parent_id,None, node_type=NodeType.leaf, edge_value=edge_value, val=node_value,operator_type=operator_type)\n",
    "            return\n",
    "\n",
    "        # find the attribute with the largest gain\n",
    "        splitting = self.__find_best_attr(sample_attrs, subset)\n",
    "        \n",
    "        # add node\n",
    "        node_id = self.__add_node(parent_id,splitting.attr,node_type=NodeType.internal, edge_value=edge_value, val=value, split_type=splitting.split_type,\n",
    "                                  split_cond=splitting.cond,operator_type=operator_type)\n",
    "        #Call tree construction recursively for each split\n",
    "        split_attrs = [x for x in attrs if x is not splitting.attr]\n",
    "        #print(split_attrs)\n",
    "        #print('New subset Splitting attributes',[x.label for x in split_attrs])\n",
    "        split_label = splitting.attr.label\n",
    "        subset_value = round(subset[self.target_attribute.label].mean())\n",
    "        if splitting.split_type == SplitType.bin:\n",
    "            ss1 = subset[ subset[split_label] <= splitting.cond]\n",
    "            ss1 = ss1.drop([split_label],axis=1)\n",
    "            ss2 = subset[ subset[split_label] > splitting.cond]\n",
    "            ss2 = ss2.drop(split_label,axis=1)\n",
    "            self.__id3(split_attrs,ss1,node_id,edge_value=subset_value,value=subset_value,operator_type=OperatorType.leq)\n",
    "            self.__id3(split_attrs,ss2,node_id,edge_value=subset_value,value=subset_value,operator_type=OperatorType.gt)\n",
    "        elif splitting.split_type  == SplitType.multi:\n",
    "            for split_cond in splitting.cond:\n",
    "                #print(f\"finding subset for category [{splitting.attr.label}-{split_cond}]\"),\n",
    "                ss = subset[subset[split_label] == split_cond]\n",
    "                ss = ss.drop([split_label],axis=1)\n",
    "                if ss.empty:\n",
    "                    print(\"WARNING!!! Empty subset not allowed\")\n",
    "                self.__id3(split_attrs,ss,node_id,edge_value=subset_value,value=subset_value,operator_type=OperatorType.eq)\n",
    "        \n",
    "    def build_model(self,subsample_trees):\n",
    "        self.model = []  # holds the decision tree model, represented as a list of nodes\n",
    "        self.__id3(self.attributes, self.data,subsample_trees=subsample_trees)\n",
    "\n",
    "    \n",
    "    def predict(self, record):\n",
    "        node = self.model[0]\n",
    "        oldNode = None\n",
    "        while node.type != NodeType.leaf:\n",
    "            oldNode = node\n",
    "            #print(f\"\\t Applying DT model for  node {node.id}-{node.attr.label}-{node.split_cond}\")\n",
    "            record_val = record[node.attr.label]\n",
    "            if node.split_type == SplitType.bin :\n",
    "                for child_idx in node.children:\n",
    "                    child_node = self.model[child_idx]\n",
    "                    if child_node.operator_type == OperatorType.leq  and record_val <= node.split_cond:\n",
    "                        node = child_node\n",
    "                        break\n",
    "                    if child_node.operator_type == OperatorType.gt  and record_val > node.split_cond:\n",
    "                        node = child_node\n",
    "                        break\n",
    "            elif node.split_type == SplitType.multi : \n",
    "                for child_node_idx in node.children:\n",
    "                    child_node = self.model[child_node_idx]\n",
    "                    if child_node.val == record_val :\n",
    "                        node = child_node\n",
    "                        break\n",
    "            if oldNode == node:\n",
    "                break\n",
    "        return node.val\n",
    "\n",
    "\n",
    "    def print_model(self, node_id=0, level=0):\n",
    "        node = self.model[node_id]\n",
    "        indent = \"  \" * level\n",
    "        if node.type == NodeType.leaf:\n",
    "            print(indent + str(node.edge_value) + \" [Leaf node] class=\" + str(node.val))\n",
    "        else:\n",
    "            cond = f\" {node.operator_type_str()} \" + str(node.split_cond) if node.attr.type == AttrType.num else f\" {node.operator_type_str()} ? \"\n",
    "            if node.type == NodeType.root:\n",
    "                print(\"[Root node] '\" + node.attr.label + \"'\" + cond)\n",
    "            else:\n",
    "                print(indent + str(node.edge_value) + \" [Internal node] '\" + node.attr.label + \"'\" + cond)\n",
    "            # print tree for child notes recursively\n",
    "            for n_id in node.children:\n",
    "                self.print_model(n_id, level + 1)\n",
    "\n",
    "class RF(object):\n",
    "    def __init__(self,data):\n",
    "        self.data = data  # training data set (loaded into memory)\n",
    "        self.forest = [] # decision trees\n",
    "        self.attributes =[]\n",
    "        self.MAX_CATEGORY_SIZE = 10\n",
    "       \n",
    "        col_length = len(self.data.columns) \n",
    "        for col_idx in range(col_length):\n",
    "            col = self.data.columns[col_idx]\n",
    "            attr_type = AttrType.num\n",
    "            stat = data.mean()\n",
    "            #Lets consider a max number of minimum values = 20 to classify the attribute as Categorical \n",
    "            colUniqueLen = len(data[col].unique())\n",
    "            if colUniqueLen <= self.MAX_CATEGORY_SIZE:\n",
    "                attr_type = AttrType.cat\n",
    "                stat = data.mode()\n",
    "            is_target = col_idx == col_length - 1 \n",
    "            attr = Attribute(col,attr_type,is_target,stat)\n",
    "            self.attributes.append(attr)\n",
    "             \n",
    "\n",
    "    def __subsampling(self, data, sample_size_ratio):\n",
    "        data_length = len(data)\n",
    "        sample_number = round(data_length * sample_size_ratio)\n",
    "        sample_indexes = np.random.randint(low=0,high= data_length, size=sample_number)\n",
    "        subsample  = data.iloc[sample_indexes,:]\n",
    "        return subsample\n",
    "        \n",
    "    def build_model(self,number_of_trees=1, sample_size_ratio=None,sample_attr_size=None):\n",
    "        for i in range(number_of_trees):\n",
    "            print(f'Creating tree # {i}...')\n",
    "            print(f'\\t Subsampling...')\n",
    "            sample = self.data\n",
    "            subsample_trees = number_of_trees > 1 \n",
    "            if  subsample_trees or not sample_size_ratio is None:\n",
    "                sample = self.__subsampling(self.data, sample_size_ratio)\n",
    "            \n",
    "            print(f'\\t Initializing...')\n",
    "            tree = DT(sample,self.attributes,sample_attr_size) \n",
    "            print(f'\\t Modelling...')\n",
    "            tree.build_model(subsample_trees)\n",
    "            self.forest.append(tree)\n",
    "            print(f'\\t Model completed {len(tree.model)}')\n",
    "            \n",
    "    def predict(self, test_data):\n",
    "        rf_predictions = pd.DataFrame(columns=['Id','SalePrice'])\n",
    "        n = test_data.shape[0]\n",
    "        print(f\"Starting predictions ({n})...\")\n",
    "        for row_idx in test_data.index:\n",
    "            row = test_data.loc[row_idx]\n",
    "            predictions = []\n",
    "            for tree in self.forest:\n",
    "                prediction = tree.predict(row) \n",
    "                predictions.append(prediction)\n",
    "            result = np.mean(predictions)\n",
    "            rf_predictions = rf_predictions.append({'Id':row_idx,'SalePrice':result},ignore_index=True)\n",
    "            #print(f\"{row_idx} of {n} = {result}\")\n",
    "        rf_predictions = rf_predictions.astype({'Id': 'int32','SalePrice':'float32'})\n",
    "        print(\"Prediction finished\")\n",
    "        return rf_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_vals(data):\n",
    "    for col in data.columns :\n",
    "        if data[col].dtype == 'object' :\n",
    "            data[col] = data[col].str.strip()\n",
    "            data[col] = data[col].fillna('NA')\n",
    "    #After a quuick check nulll data was found           \n",
    "    # x=data.isna()\n",
    "    # x = x.sum()\n",
    "    # y = x[x > 0].sort_values()\n",
    "    # types = data[y.keys()].dtypes\n",
    "    # The rest of the values consider NA as an acceptable category, for these it will be\n",
    "    # Necessary to fill the nulls\n",
    "    null_cols = ['Electrical','MasVnrArea','GarageYrBlt','LotFrontage']\n",
    "    for col in null_cols:\n",
    "        data_col = data[col]\n",
    "        null_rows = data_col.isnull() \n",
    "        non_nulls = data_col[null_rows == False]\n",
    "        mean_val = None\n",
    "        if data[col].dtype == 'object':\n",
    "            mean_val = non_nulls.value_counts().keys()[0]\n",
    "        else:\n",
    "            mean_val = non_nulls.mean()\n",
    "        data[col].fillna(mean_val,inplace=True)\n",
    "\n",
    "    for col in data.columns:\n",
    "        if data[col].dtype == 'object':\n",
    "            data[col] = data[col].astype('category')\n",
    "            data[col]  = data[col].cat.codes\n",
    "\n",
    "def transform_data(data):\n",
    "    data['WoodDeckSF'] = data['WoodDeckSF'].apply(lambda x: 0 if x == 0  else 1)\n",
    "    data['OpenPorchSF'] = data['OpenPorchSF'].apply(lambda x: 0 if x == 0  else 1)\n",
    "    data['LotFrontage'] = data['LotFrontage'].apply(lambda x: round(x/10))\n",
    "    data['YearBuilt'] = data['YearBuilt'].apply(lambda x: round(x/1000,2))\n",
    "    data['GarageYrBlt'] = data['GarageYrBlt'].apply(lambda x: round(x/1000,2))\n",
    "    data['YearRemodAdd'] = data['YearRemodAdd'].apply(lambda x: round(x/1000,2))\n",
    "    data['GrLivArea'] = data['GrLivArea'].apply(lambda x: round(x/100))\n",
    "    data['BsmtFinSF1'] = data['BsmtFinSF1'].apply(lambda x: 0 if x == 0 else 1)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def load_train_data(file_name,plot=False,filter=False):\n",
    "    data = pd.read_csv(file_name,index_col='Id')\n",
    "    fill_missing_vals(data)\n",
    "    data_d = data.describe()\n",
    "    ## Finding columns with the mayority of values = to 0\n",
    "    zero_features = data_d.iloc[6,:] == 0\n",
    "    filtered_zero_features = zero_features[zero_features]\n",
    "    data.drop(filtered_zero_features.keys(),axis=1, inplace=True)\n",
    "    if plot:\n",
    "        data.hist(figsize=(20,20), bins=20)\n",
    "        plt.show()\n",
    "    corr = data.corr()\n",
    "    corr_values = corr['SalePrice'].sort_values(ascending=False)\n",
    "    if plot:\n",
    "        fig,ax = plt.subplots(figsize=(12,9))\n",
    "        sns.heatmap(corr,vmax=.8,square=True)\n",
    "    ##Let's consider now correlation values above 0.3, which is a good value representing correlation\n",
    "    good_correlation = corr_values >= 0.3\n",
    "    feature_values = good_correlation[good_correlation]\n",
    "    \n",
    "    drop_columns = [c for c in  data.columns if c not in feature_values  ]\n",
    "    # drop_columns.append('MasVnrArea')\n",
    "    # drop_columns.append('2ndFlrSF')\n",
    "    # drop_columns.append('TotRmsAbvGrd')\n",
    "     \n",
    "\n",
    "    data.drop(drop_columns,axis=1, inplace=True)\n",
    "    \n",
    "    corr = data.corr()\n",
    "    corr_values = corr['SalePrice'].sort_values(ascending=False)\n",
    "    if plot :\n",
    "        sns.set(font_scale=1.25)\n",
    "        fig,ax = plt.subplots(figsize=(12,9))\n",
    "        sns.heatmap(corr,vmax=.8,square=True,annot=True)\n",
    "\n",
    "\n",
    "    #remove outliers\n",
    "    #data = data[data['SalePrice'] <= 225000]\n",
    "    #data = data[data['SalePrice'] >= 100000]\n",
    "\n",
    "    data = data[data['TotalBsmtSF'] <= 2500]\n",
    "    data = data[data['TotalBsmtSF'] >= 600]\n",
    "    data = data[data['1stFlrSF'] <= 1800]\n",
    "\n",
    "    #data = data[data['MasVnrArea'] <= 500]\n",
    "    #data = data[data['MasVnrArea'] < 300]\n",
    "    # data = data[data['GrLivArea'] <= 1550]\n",
    "    # data = data[data['GrLivArea'] > 800]\n",
    "    data = data[data['FullBath'] > 0]\n",
    "    data = data[data['FullBath'] < 3]\n",
    "    # data = data[data['TotRmsAbvGrd'] > 3]\n",
    "    # data = data[data['TotRmsAbvGrd'] < 9]\n",
    "\n",
    "    # data = data[data['OverallQual'] > 3]\n",
    "    # data = data[data['YearBuilt'] >= 1950]\n",
    "    # data = data[data['YearBuilt'] < 2006]\n",
    "    # data = data[data['YearRemodAdd'] < 2005]\n",
    "    # data = data[data['LotFrontage'] < 90]\n",
    "    # data = data[data['Foundation'] <= 2]\n",
    "    # data = data[data['BsmtFinSF1'] <= 1100]\n",
    "    # data = data[data['GarageCars'] < 3]\n",
    "    # data = data[data['WoodDeckSF'] < 350]\n",
    "\n",
    "    \n",
    "\n",
    "    #data = transform_data(data)\n",
    "    \n",
    "    # for col in data.columns:\n",
    "    #     if col != 'SalePrice':\n",
    "    #         data[col] = np.around( np.log1p(data[col]),1)\n",
    "    # if not filter:\n",
    "    #     return data\n",
    "    # for col in data.columns:\n",
    "    #     unique_vals = data[col].unique()\n",
    "    #     #print(col, len(unique_vals))\n",
    "    #     if len(unique_vals) > 600:\n",
    "    #         d = data[col].describe()\n",
    "    #         mean = d['mean'] \n",
    "    #         std = d['std']\n",
    "    #         min = mean - std\n",
    "    #         max = mean + std\n",
    "    #         data = data[data[col] >= min]\n",
    "    #         data = data[data[col] <= max]\n",
    "    return data\n",
    "    # fig,ax = plt.subplots(figsize=(12,9))\n",
    "    # sns.heatmap(corr,vmax=.8,square=True)\n",
    "\n",
    "def load_test_data(file_name,keep_cols): \n",
    "    test_data = pd.read_csv(file_name,index_col='Id')\n",
    "    data_cols = test_data.columns\n",
    "    drop_cols = [col for col in data_cols if col not in keep_cols ]\n",
    "    fill_missing_vals(test_data)\n",
    "    test_data.drop(drop_cols,axis=1, inplace=True)\n",
    "    #test_data = transform_data(test_data)\n",
    "\n",
    "\n",
    "\n",
    "    # for col in data.columns:\n",
    "    #     if col != 'SalePrice':\n",
    "    #         test_data[col] = np.around( np.log1p(test_data[col]),1)\n",
    "\n",
    "    return test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [os.path.join(dp, f) for dp, dn, filenames in os.walk(os.getcwd()) for f in filenames]\n",
    "print(result) \n",
    "# print(os.getcwd())\n",
    "\n",
    "\n",
    "\n",
    "path = \"\"\n",
    "path = \"user/assignment2/\"\n",
    "train_file_name = path + \"data/housing_price_train.csv\"\n",
    "test_file_name = path + \"data/housing_price_test.csv\"\n",
    "\n",
    "submission_file_name = path + \"submission.csv\"\n",
    "train_data = load_train_data(train_file_name)\n",
    "test_data = load_test_data(test_file_name,train_data.columns) \n",
    "rf =  RF(train_data)\n",
    "# for col in train_data.columns:\n",
    "#     if col != 'SalePrice': \n",
    "#         train_data.plot(x=col, y='SalePrice', style='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Creating tree # 0...\n\t Subsampling...\n\t Initializing...\n\t Modelling...\n\t Model completed 17853\nStarting predictions (1560)...\nPrediction finished\n"
    }
   ],
   "source": [
    "rf.build_model()\n",
    "test_results = rf.predict(test_data)\n",
    "test_results.to_csv( submission_file_name,index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}